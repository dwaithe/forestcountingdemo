import scipy
from scipy import io
from vigra import superfeatures as sf
import vigra
import pylab
import numpy as np
import numpy
import signal,sys
from scipy import ndimage
signal.signal(signal.SIGINT, signal.SIG_DFL)
import random
import os,glob
from joblib import Parallel,delayed
from spatialAverage import *
from utils import *
from matplotlib.backends.backend_pdf import PdfPages
from pool import ThreadPool
from threading import Lock
import gc
import pylab
from scipy import ndimage as nd
import gc
pool = ThreadPool(8)
tlock=Lock()
from time import time
def formMultispectral(img):
    #Extract the featues on the image and compute multispectral image
    img=img.copy()
    img=img.astype(np.float32)
    
    a=[]
    blue=img[:,:,0]
    dt=img[:,:,1]
    
    a.append(blue)
    a.append(dt)
    
    N=3
    base=1
    r2=2
    for n in range(N):
        f=base*(r2**n)
        #gg= vigra.filters.gaussianGradientMagnitude(blue,f).view(np.ndarray)
        #a.append(gg)
        te=vigra.filters.structureTensorEigenvalues(blue,f,f*2).view(np.ndarray)
        a.append(te)
        
        lg=vigra.filters.laplacianOfGaussian(blue,f).view(np.ndarray)
        a.append(lg)
        #pylab.imshow(te[:,:,0])
        #pylab.show()
        #pylab.imshow(lg)
        #pylab.show()
    res=np.dstack(a)

    nchannels=res.shape[-1]
    #correct prespective distortion
    correction=np.repeat(distortion,nchannels,axis=2)
    res*=correction
    
        
    return res

def testOnImg(rf,testimg,gtdots,pw,predictions,Id,fileh):
    ih,iw=gtdots.shape
    mimg=formMultispectral(testimg)
    fMatrixTest,pos=extract_dense(mimg,pw=pw)
    
    pred=rf.predict(fMatrixTest.astype(np.float64))
    
    
    resImg=spatialAverage((ih,iw),pos,pred,pw=pw) #spatial average of the prediction
    
    
    
    gtdots=gtdots*roadImage
    resImg=resImg*roadImage
    
    
    #Store the result
    dx=dy=pw/2
    #Remove border region due to feature computation 3*3=12 pixels max sigma=2 4*4=16
    npred=resImg[dy*6:ih-dy*6,dx*6:iw-dx*6].sum()
    ntrue=gtdots[dy*6:ih-dy*6,dx*6:iw-dx*6].sum()
    err=np.abs(npred-ntrue)
    
    
    tlock.acquire()
    print "Image = %d , ntrue = %f , npred = %f , err = %f \n"%(Id,ntrue,npred,err)

    fileh.write('%f '%ntrue)
    fileh.write('%f '%npred)
    fileh.write('%f\n'%err)
    pylab.subplot(2,2,1)
    pylab.imshow(testimg[dy*6:ih-dy*6,dx*6:iw-dx*6,2])
    
    pylab.subplot(2,2,2)
    pylab.imshow(roadImage[dy*6:ih-dy*6,dx*6:iw-dx*6])
        
    pylab.subplot(2,2,3)
    pylab.imshow(resImg[dy*6:ih-dy*6,dx*6:iw-dx*6].view(np.ndarray).astype(np.float32))
    pylab.title("estimated count %f "%npred)
    #pylab.colorbar()
    print np.max(resImg), np.min(resImg)
    pylab.subplot(2,2,4)
    #pylab.colorbar()
    pylab.imshow(gtdots[dy*6:ih-dy*6,dx*6:iw-dx*6].view(np.ndarray).astype(np.float32))
    print np.max(gtdots), np.min(gtdots)
    pylab.title("true count %f "%ntrue)
    pylab.savefig("bosh/%d.png"%Id)
    predictions[Id]=(resImg,ntrue,npred,err)
    tlock.release()
    
    
    
def getTrainingTestSet(imgs,dots,mode,**kwargs):
    if mode=='maximal':
        start=600
        stop=1400
        skip=5
    elif mode=='downscale':
        start=1205
        stop=1601
        skip=5
    elif mode=='upscale':
        start=805
        stop=1101
        skip=5
    elif mode=='minimal':
        start=604
        stop=1361
        skip=80
    
    traingindex=range(start,stop,skip)
    testindex=range(0,start)+range(stop,len(imgs))
    
    trainImgs=[imgs[i] for i  in traingindex]
    trainDots=[dots[i] for i in traingindex]
    testImg=[imgs[i] for i in testindex]
    testDots=[dots[i] for i in testindex]
    return trainImgs,trainDots,testImg,testDots
    
        

def extractFeaturesAndPostraining(dot,img,pw,Nr):    
    opatch,pos=extract_at_random(dot,pw=pw,N=Nr)
    mimg=formMultispectral(img)
    fMatrix=extract_at_pos(mimg,pos,pw=pw)
    return opatch,fMatrix
    
def runExperiment(**kwargs):#pw,treeCount,features_per_node,min_split_node_size,max_depth,N,Nr,sigmadots,expId,mode):



    #Split the dataset in training imgs and dots

    folderResults=os.path.join(baseFolder,'experiment%.5d_800'%Id)
    if not os.path.exists(folderResults): os.mkdir(folderResults)
    
    nemeFileResults=os.path.join(folderResults,'log.txt')
    fh=open(nemeFileResults,'wa')
    fh.write(str(kwargs))
    fh.write('\n')
    
    print "computing features on the the training set",Nr
    tmp=Parallel(8)(delayed(extractFeaturesAndPostraining)(dot,img,pw,Nr) for img,dot in zip(trainImgs,trainDots))
    fMatrix=[fMatrix for _,fMatrix in tmp]
    opatches=[opatch for opatch,_ in tmp]
    fMatrix=np.vstack(fMatrix).astype(np.float64)
    opatches=np.vstack(opatches).astype(np.float64)
    
    
    #Train the regression forest
    rf=sf.RandomRegressionForest(treeCount = treeCount,features_per_node=features_per_node)
    
    
    
    print "Training Set features Shape",fMatrix.shape
    print "Training Set Labels Shape",opatches.shape
    print "Start training"
    rf.learnRF(fMatrix,opatches,min_split_node_size=min_split_node_size,max_depth=max_depth,verbose=True)
        
    
    predictions=np.ndarray(len(testImg),dtype=object)
    for k in range(len(testImg)):
        testimg=testImg[k]
        gtdots=testDots[k]
        pool.add_task(testOnImg,rf,testimg,gtdots,pw,predictions,k,fh)
    
    pool.wait_completion()
    
    fh.close()
    errors=[err for resImg,ntrue,npred,err in predictions]
    print "TOTAL ERROR :", np.mean(errors)
    gc.collect()
    return np.mean(errors),kwargs
    


def NormalizeImgs(imgs):
    vol=np.dstack(imgs)
    m=np.median(vol, axis=-1)
    imgs=[img-m for img in imgs]
    return imgs

def computeTDerivative(imgs):
    vol=np.dstack(imgs)
    #vol2=np.roll(vol, shift=1, axis=2)
    #vol=vol2-vol
    
    vol=nd.filters.sobel(vol, axis=2)#, output, mode, cval)
    res=np.dsplit(vol, len(imgs))
    res=[el.squeeze() for el in res]
    return res

    
if __name__=="__main__":
    #Test in the clssical way
    
    roadImage=vigra.impex.readImage("data/road.png").view(np.ndarray).astype(np.float32).swapaxes(0,1)
    roadImage=np.where(roadImage!=0,1,0)
    
    distortion=np.loadtxt('data/distortion.txt', delimiter=',')
    #correct for prespective distortion
    dh,dw=distortion.shape
    distortion=np.square(np.reshape(distortion,(dh,dw,1)))
    
    
    
    stop=-1
    dataFolder='data/people/'
    imgs=importImagesFolder(dataFolder,'*people.png',stop=stop,verbose=False)
    dots=importImagesFolder(dataFolder,'*dots.png',stop=stop,verbose=False)
    
    
    imgs2=NormalizeImgs(imgs)
    dt=computeTDerivative(imgs)
    
    for k,img in enumerate(imgs2):
        imgs[k]=np.dstack([img,dt[k],imgs[k]])
        
    
    
    
    sigmadots=3
    for k,dot in enumerate(dots):
        dot=dot[:,:]
        dot/=255
        dot=vigra.filters.gaussianSmoothing(dot, sigmadots)
        dots[k]=dot

    
    training_mode=sys.argv[1]
    print "TRAININNG MODE ...............",training_mode
    #training_mode='maximal'
    #training_mode='downscale'
    #training_mode='upscale'
    #training_mode='minimal'
    
    
    
    treeCount=20;
    pw=7 #Target patch width
    min_split_node_size=10
    max_depth=15
    N=32 #Number of training images
    Nr=100#Number of randomly extracted training patches
    downsample=False
    
    
    baseFolder='results/people/'
    
    nameFile=os.path.join(baseFolder,'fred_test_bosh_%s_.txt'%training_mode)
    f=open(nameFile,"a")
    
    
    
    f.write("##################### NEW SESSION ################# with distortion correction \n")
    f.close()
    
    print "extracting the training set"
    trainImgs,trainDots,testImg,testDots=getTrainingTestSet(imgs,dots,mode=training_mode)
    if downsample:
        testImg=testImg[::5]
        testDots=testDots[::5]
        print "LEN Test Imgs = ", len(testImg)
    
    print "start running"
    
    
    rNr=[500]
    
    if training_mode=="minimal":
        Id=0
        #rNr=[600]
    elif training_mode=="maximal":
        Id=1000
    elif training_mode=="downscale":
        Id=2000
    elif training_mode=="upscale":
        Id=3000
    
    
    Id=33
    
    for treeCount in [30]:
        for max_depth in [10]:
            for min_split_node_size in [20]:
                for features_per_node in [pw*pw*3]:
                    #for Nr in [1]:
                    for Nr in rNr:
                        for i in range(3):
                            f=open(nameFile,"a")
                            Id+=1
                            
                            
                            s=time()
                            res=runExperiment(pw=pw,treeCount=treeCount,features_per_node=features_per_node,
                                              min_split_node_size=min_split_node_size,max_depth=max_depth,
                                              N=N,Nr=Nr,sigmadots=sigmadots,Id=Id,baseFolder=baseFolder,
                                              mode=training_mode,downsample=downsample)
                        #print "ABSOLUTE ERRRRRORRRRR>...", res[-1]#text="\n ID %d \n pw %f \n treeCount %f \n features_per_node %f \n min_split_node_size %f \n \
                        #N training imgs %f \n Npathces %f \sigmadots %f \n meanErr %f \n"%(expId,pw,treeCount,features_per_node,min_split_node_size,N,Nr,sigmadots,res[-1])
                            
                            text="MEAN ABSOLUTE ERROR %f\n"%res[0]
                            text=text+str(res[1])
                            text=text+"\n"
                            
                            f.write(text)
                            f.write("time %f\n"%(time()-s))
                            f.close()
        
    
    
